
# Drone-Based Weed Detection and Selective Spraying üöÅüåø

An end-to-end edge-AI system for **real-time weed detection and selective spraying**
using **YOLOv8**, **NVIDIA Jetson Nano**, and **DroneKit-based flight controller integration**.

This project demonstrates a full pipeline from **data preparation and model training**
to **onboard inference and real-world actuation**.

---

## Problem Statement

Invasive weeds such as **Lantana** and **Parthenium** significantly reduce crop yield
and require precise treatment. Manual spraying is inefficient, labor-intensive,
and often damages surrounding crops.

---

## Solution Overview

This project implements a **drone-mounted vision system** that:

- Detects target weeds in real time using deep learning
- Runs inference on **Jetson Nano (edge device)**
- Communicates with the **flight controller via DroneKit (MAVLink)**
- Triggers a **sprayer/relay selectively**, avoiding non-target areas

---

## System Pipeline

Data Collection
‚Üì
Annotation (LabelMe)
‚Üì
LabelMe ‚Üí YOLO Conversion
‚Üì
Dataset Validation & Train/Val/Test Split
‚Üì
Preprocessing (Image Quality Checks)
‚Üì
YOLOv8 Training (Built-in Augmentation)
‚Üì
Model Evaluation (mAP, Precision, Recall)
‚Üì
Jetson Nano Inference (Real-Time)
‚Üì
DroneKit ‚Üí Flight Controller (AUX Servo)
‚Üì
Selective Spraying


---

## Tech Stack

- **Programming Language**: Python
- **Model**: YOLOv8 (Nano / Small)
- **Frameworks**: Ultralytics YOLO, OpenCV
- **Edge Device**: NVIDIA Jetson Nano
- **Flight Control**: Pixhawk / Cube (ArduPilot)
- **Communication**: DroneKit (MAVLink)
- **Optimization**: TensorRT
- **Annotation Tool**: LabelMe

---

## Repository Structure

weed-detection-drone/
‚îú‚îÄ‚îÄ data_collection/        # Scripts for dataset collection (research use)
‚îÇ   ‚îî‚îÄ‚îÄ scrape_images.py
‚îÇ
‚îú‚îÄ‚îÄ tools/                  # Annotation & preprocessing utilities
‚îÇ   ‚îú‚îÄ‚îÄ labelme_to_yolo.py
‚îÇ   ‚îú‚îÄ‚îÄ visualize_yolo_annotations.py
‚îÇ   ‚îú‚îÄ‚îÄ split_dataset.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocess_images.py
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ training/               # Model training and fine-tuning
‚îÇ   ‚îú‚îÄ‚îÄ train_yolo.py
‚îÇ   ‚îî‚îÄ‚îÄ finetune_yolo.py
‚îÇ
‚îú‚îÄ‚îÄ evaluation/             # Model evaluation and metrics
‚îÇ   ‚îî‚îÄ‚îÄ evaluate_yolo.py
‚îÇ
‚îú‚îÄ‚îÄ inference/              # Deployment & real-time inference
‚îÇ   ‚îú‚îÄ‚îÄ infer_image.py
‚îÇ   ‚îú‚îÄ‚îÄ infer_video.py
‚îÇ   ‚îú‚îÄ‚îÄ jetson_dronekit_infer.py
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ optimization/           # TensorRT optimization for Jetson
‚îÇ   ‚îú‚îÄ‚îÄ export_tensorrt.sh
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ outputs/                # Sample outputs (images, videos, logs)
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ videos/
‚îÇ   ‚îî‚îÄ‚îÄ logs/
‚îÇ
‚îú‚îÄ‚îÄ dataset.yaml            # YOLO dataset configuration
‚îú‚îÄ‚îÄ README.md               # Project overview and pipeline
‚îî‚îÄ‚îÄ .gitignore




---

## Dataset

- **Classes**
  - `lantana`
  - `parthenium`
- Dataset prepared using:
  - Manual image collection
  - Web-based reference images (research use)
  - LabelMe polygon annotations
- Converted to YOLO format and split into train/validation/test

> **Note**: Full dataset is not included due to size and IP constraints.

---

## Preprocessing & Augmentation

### Preprocessing
- Image validation and corruption checks
- Optional downscaling of extremely large images
- Labels remain untouched to avoid misalignment

### Augmentation
- YOLOv8 built-in augmentation:
  - Mosaic
  - MixUp
  - HSV jitter
  - Scaling and flipping

No offline augmentation is required for standard training.

---

## Model Training

- Base model: `yolov8n.pt` / `yolov8s.pt`
- CPU-friendly settings for experimentation
- Early stopping via **patience**
- Reproducible training using `dataset.yaml`

---

## Model Evaluation

Evaluation performed on validation data using:
- **mAP@0.5**
- **mAP@0.5:0.95**
- **Precision**
- **Recall**
- Confusion matrix and prediction visualizations

Artifacts are saved automatically under `runs/detect/val/`.

---

## Jetson Nano Inference

- Real-time inference using USB / CSI camera
- Optimized input size for FPS vs accuracy trade-off
- Supports:
  - PyTorch inference
  - TensorRT-optimized inference (`.engine`)

---

## Flight Controller Integration

- Jetson communicates with the flight controller using **DroneKit**
- AUX servo outputs are used to control:
  - Relay
  - Pump
  - Sprayer

### Trigger Logic
- If target weed detected ‚Üí **SPRAY ON**
- If no target detected ‚Üí **SPRAY OFF**
- Ensures selective and controlled spraying

---

## TensorRT Optimization

- YOLO model exported to TensorRT on Jetson Nano
- Provides significantly higher FPS and lower latency
- Engine files are **not committed** (hardware-specific)
- Export scripts are provided for reproducibility

---

## Demo & Outputs

Due to GitHub limitations on video preview:



‚ñ∂Ô∏è **Demo Video**:  
lantana - https://drive.google.com/drive/folders/1mAF7guROvbUGA6PAaTVY89e-JJCryNdG?usp=sharing
parthenium - https://drive.google.com/drive/folders/1XYsGFoF6a56EdG9odOfs-lNb-xFzzvXN?usp=sharing

---

## Safety & Reliability Considerations

- Confidence thresholding to reduce false triggers
- Servo control via flight controller (not direct GPIO)
- Separation of perception (AI) and actuation (control)
- Easy integration of debounce and failsafe logic

---

## Key Learnings

- End-to-end ML system design (not just model training)
- Edge AI deployment constraints and optimization
- Real-time integration between AI and embedded systems
- Importance of clean pipelines and reproducibility

---

## Future Improvements

- Pixel-wise weed segmentation
- Multi-class weed detection
- GPS-aware spraying logic
- Autonomous navigation without GPS
- Advanced failsafe and debounce mechanisms

---

## Author

**Adarsh Anil**  
AI & Computer Vision Engineer  
Drone-based Edge AI Systems  

---

## License

This project is released under the **MIT License**.
